{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nfrom tqdm import tqdm_notebook as tqdm\nimport pandas as pd\nfrom pathlib import Path\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/embeddings\"))\n",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['test.csv', 'train.csv', 'sample_submission.csv', 'embeddings']\n['wiki-news-300d-1M', 'glove.840B.300d', 'paragram_300_sl999', 'GoogleNews-vectors-negative300']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#load the data:\ntrain = pd.read_csv(Path(\"../input/train.csv\"))\ntest = pd.read_csv(Path('../input/test.csv'))\nsub = pd.read_csv(Path('../input/sample_submission.csv'))\ny = train.target.values\nprint(\"done reading input files\")\nprint(test.head())\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "done reading input files\n                    qid                                      question_text\n0  00014894849d00ba98a9  My voice range is A2-C5. My chest voice goes u...\n1  000156468431f09b3cae           How much does a tutor earn in Bangalore?\n2  000227734433360e1aae  What are the best made pocket knives under $20...\n3  0005e06fbe3045bd2a92  Why would they add a hypothetical scenario tha...\n4  00068a0f7f41f50fc399   What is the dresscode for Techmahindra freshers?\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c4bb4efa9de5ca940f661093a6a7554ca6cd27f"
      },
      "cell_type": "code",
      "source": "# split the dataset into training and validation datasets \ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['question_text'], train['target'])\n\n#train_x = train['question_text']\n#train_y = train['target']\n\ntest_x = test['question_text']",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2f4a091228f9fd76d042352c66ad0e61fd66b80"
      },
      "cell_type": "code",
      "source": "# load the pre-trained word-embedding vectors \n# needs about 999995it to finish\nembeddings_index = {}\nfor i, line in enumerate(tqdm(open('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))):\n    values = line.split()\n    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e6eebc603354f8b883cff877afb1fca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4cc2fa2b611dba2f4b207514d572a22032b42dcd"
      },
      "cell_type": "code",
      "source": "# create a tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(train['question_text'])\nword_index = token.word_index",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6eb5f812e00a34d109d9bf838c985d5cb0b3c48"
      },
      "cell_type": "code",
      "source": "# convert text to sequence of tokens and pad them to ensure equal length vectors \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\ntest_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x))",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "05ba6b25762c2366306371335f292c3d547f4091"
      },
      "cell_type": "code",
      "source": "\n# create token-embedding mapping\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=222161), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b16a1f456c4c5ca25f7fc799b2680f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d03e04ddf3b138f194d530211f604ec64af41c39"
      },
      "cell_type": "code",
      "source": "def create_cnn():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the convolutional Layer\n    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n    # Add the pooling Layer\n    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dbc3a383212c3ba292ead53eb47d3ccfeb9e8a2"
      },
      "cell_type": "code",
      "source": "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, valid_y)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86156854fdebef02c4f2d8b236e9351158f80692"
      },
      "cell_type": "code",
      "source": "classifier = create_cnn()\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"CNN, Word Embeddings\",  accuracy)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/1\n979591/979591 [==============================] - 823s 840us/step - loss: 0.1232\nCNN, Word Embeddings 0.9382937607761591\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d2fd047aa1b45be7f0ec064da83887a328aa20f"
      },
      "cell_type": "code",
      "source": "cnn_model = create_cnn()\ncnn_model.fit(train_seq_x, train_y)\npredictions = cnn_model.predict(valid_seq_x, verbose=1)\npredictions = predictions.argmax(axis=-1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3462a8f2ddee05037e720940b3f03d75508ba5a2"
      },
      "cell_type": "code",
      "source": "n = sub.columns[1]\nsub.drop(n, axis = 1, inplace = True)\nsub[n] = predictions\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}