{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "#sub = pd.read_csv(Path('../input/sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    bag_of_words = [x for x in wordpunct_tokenize(text)]\n",
    "\n",
    "    features = []\n",
    "    #**************************************************************************\n",
    "    ## Countable/statistical features\n",
    "    #**************************************************************************\n",
    "    # Example feature 1: count the number of words\n",
    "    num_words = len(bag_of_words)\n",
    "    features.append(num_words)\n",
    "    \n",
    "    # words without vowels\n",
    "    features.append(len([word for word in bag_of_words if not re.search('[aeiou]', word.lower(), re.I)]))\n",
    "    \n",
    "    # number of characters including whitespace\n",
    "    features.append(len(text))\n",
    "\n",
    "    # number of characters without whitespace\n",
    "    features.append(len(text.replace(\" \", \"\")))\n",
    "\n",
    "    # punctuation count\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    features.append(count(text, set(string.punctuation)))\n",
    "\n",
    "    # number of numbers\n",
    "    features.append(sum(c.isdigit() for c in text))\n",
    "\n",
    "    # number of alpha chars\n",
    "    features.append(sum(c.isalpha() for c in text))\n",
    "\n",
    "    # number of spaces\n",
    "    features.append(sum(c.isspace() for c in text))\n",
    "    \n",
    "    # Commas per sentence\n",
    "    features.append(bag_of_words.count(','))    # TODO alterative text.count(\",\") <- see what is faster\n",
    "\n",
    "    # Semicolons per sentence\n",
    "    features.append(bag_of_words.count(';'))\n",
    "\n",
    "    # Two/three continuous punctuation count\n",
    "    features.append(len(re.findall('(\\!|\\?){2,}', text)))\n",
    "\n",
    "    # number of all caps words\n",
    "    features.append(sum(1 for word in bag_of_words if word.isupper()))\n",
    "    \n",
    "    # number of selfe reference\n",
    "    features.append(bag_of_words.count(\"I\") + bag_of_words.count(\"me\") + bag_of_words.count(\"myslef\") + bag_of_words.count(\"my\") / num_words)\n",
    "\n",
    "    # number of small letters 'i' instead of 'I'\n",
    "    features.append(bag_of_words.count(\"i\"))\n",
    "\n",
    "    # number of sentences that have no space after a full stop\n",
    "    features.append(len(re.findall('((\\.|\\?|\\!|\\:)\\w+)', text)))\n",
    "\n",
    "    # number of questions\n",
    "    features.append(len(re.findall('\\?', text)))\n",
    "\n",
    "    # number of exclamation marks\n",
    "    features.append(len(re.findall('\\!', text)))\n",
    "    \n",
    "    # question starts with number\n",
    "    features.append(1 if text[0].isdigit() else 0)\n",
    "    \n",
    "    # number of he\n",
    "    features.append(bag_of_words.count(\"he\") + bag_of_words.count(\"He\"))\n",
    "\n",
    "    # number of she\n",
    "    features.append(bag_of_words.count(\"she\") + bag_of_words.count(\"She\"))\n",
    "\n",
    "    # number of he/she\n",
    "    features.append(bag_of_words.count(\"he/she\") + bag_of_words.count(\"He/she\"))\n",
    "    \n",
    "    #**************************************************************************\n",
    "    # POS based features\n",
    "    #**************************************************************************\n",
    "    pos_tags = [pos_tag[1] for pos_tag in nltk.pos_tag(bag_of_words)]\n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS',\n",
    "                'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$',\n",
    "                'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN',\n",
    "                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "    counted_pos = []\n",
    "    for part_of_speech in pos_list:\n",
    "        counted_pos.append(pos_tags.count(part_of_speech))\n",
    "\n",
    "    [features.append(i) for i in counted_pos]\n",
    "    \n",
    "    # Blacklisted words\n",
    "    insincere_words = ['penis', 'dick', 'gay', 'fuck', 'sex', 'suck', 'bisexual', 'idiot', 'moron', 'hoe', 'bitch', 'trump', 'putin']\n",
    "    lower_bag = [word.lower() for word in bag_of_words]\n",
    "    counted_rude = []\n",
    "    for rude in insincere_words:\n",
    "        counted_rude.append(lower_bag.count(rude))\n",
    "\n",
    "    features.extend(counted_rude)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training and evaluation loop\n",
    "def train_and_evaluate__model(model, data_train, labels_train, data_valid, labels_valid, data_test, labels_test):\n",
    "    start = time.time()\n",
    "    model.fit(data_train, labels_train)\n",
    "    end = time.time()\n",
    "    print('Training time: ', end - start)\n",
    "    pred_val = model.predict(data_valid)\n",
    "    #pred_test = model.predict(data_test)\n",
    "    \n",
    "    recall = sklearn.metrics.recall_score(labels_valid, pred_val)\n",
    "    print(\"Recall validation: %f\" % recall)\n",
    "    #recall = sklearn.metrics.recall_score(labels_test, pred_test)\n",
    "    #print(\"Recall test: %f\" % recall)\n",
    "\n",
    "    precision = sklearn.metrics.precision_score(labels_valid, pred_val)\n",
    "    print(\"Precision validation: %f\" % precision)\n",
    "    #precision = sklearn.metrics.precision_score(labels_test, pred_test)\n",
    "    #print(\"Precision test: %f\" % precision)\n",
    "\n",
    "    f1_score = sklearn.metrics.f1_score(labels_valid, pred_val)\n",
    "    print(\"F1-score validation: %f\" % f1_score)\n",
    "    #f1_score = sklearn.metrics.f1_score(labels_test, pred_test)\n",
    "    #print(\"F1-score test: %f\" % f1_score)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test split\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "print(\"Train/test split\")\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(train['question_text'], train['target'], test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1175509/1175509 [30:08<00:00, 650.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 130613/130613 [03:19<00:00, 655.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract the features\n",
    "train_features = list(map(extract_features, tqdm(list(train_x))))\n",
    "test_features = list(map(extract_features, tqdm(list(test_x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create the classifier\n",
    "classifier = naive_bayes.MultinomialNB()\n",
    "#classifier = ensemble.RandomForestClassifier(max_depth=5, n_estimators=50, max_features='auto')\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, GNB and PCA.\n",
    "#classifier = make_pipeline(StandardScaler(), PCA(n_components='mle'), naive_bayes.MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "#tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "#tfidf_vect_ngram_chars.fit(list(train_x))\n",
    "#xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "#xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x) \n",
    "#xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test['question_text'])\n",
    "\n",
    "#for i, row in enumerate(xtrain_tfidf_ngram_chars):\n",
    "#    train_features[i].append(row)\n",
    "    \n",
    "#for i, row in enumerate(xvalid_tfidf_ngram_chars):\n",
    "#    test_features[i].append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 10\n",
      "Training time:  0.7169930934906006\n",
      "Recall validation: 0.267850\n",
      "Precision validation: 0.412850\n",
      "F1-score validation: 0.324906\n",
      "Running Fold 2 / 10\n",
      "Training time:  0.7339897155761719\n",
      "Recall validation: 0.267437\n",
      "Precision validation: 0.412827\n",
      "F1-score validation: 0.324595\n",
      "Running Fold 3 / 10\n",
      "Training time:  0.7010154724121094\n",
      "Recall validation: 0.268262\n",
      "Precision validation: 0.413486\n",
      "F1-score validation: 0.325407\n",
      "Running Fold 4 / 10\n",
      "Training time:  0.7459926605224609\n",
      "Recall validation: 0.275554\n",
      "Precision validation: 0.419213\n",
      "F1-score validation: 0.332531\n",
      "Running Fold 5 / 10\n",
      "Training time:  0.705939531326294\n",
      "Recall validation: 0.257257\n",
      "Precision validation: 0.397281\n",
      "F1-score validation: 0.312291\n",
      "Running Fold 6 / 10\n",
      "Training time:  0.7189927101135254\n",
      "Recall validation: 0.260146\n",
      "Precision validation: 0.402597\n",
      "F1-score validation: 0.316062\n",
      "Running Fold 7 / 10\n",
      "Training time:  0.7199928760528564\n",
      "Recall validation: 0.270739\n",
      "Precision validation: 0.410513\n",
      "F1-score validation: 0.326287\n",
      "Running Fold 8 / 10\n",
      "Training time:  0.7369909286499023\n",
      "Recall validation: 0.265374\n",
      "Precision validation: 0.401959\n",
      "F1-score validation: 0.319688\n",
      "Running Fold 9 / 10\n",
      "Training time:  0.7479794025421143\n",
      "Recall validation: 0.270876\n",
      "Precision validation: 0.412442\n",
      "F1-score validation: 0.326995\n",
      "Running Fold 10 / 10\n",
      "Training time:  0.6939923763275146\n",
      "Recall validation: 0.271977\n",
      "Precision validation: 0.409996\n",
      "F1-score validation: 0.327020\n",
      "Mean F1 score is 0.3270200975932511 SD is 0.0\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "n_folds = 10\n",
    "data = np.asarray(train_features)\n",
    "labels = np.asarray(train_y)\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "f1_scores = []\n",
    "for i, (train, valid) in enumerate(skf.split(data, labels)):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    # oversampling of insincere questions\n",
    "    # ros = RandomOverSampler(random_state=1)\n",
    "    # train_data, train_labels = ros.fit_resample(data[train], labels[train])\n",
    "    train_data = data[train]\n",
    "    train_labels = labels[train]\n",
    "    \n",
    "    validation_data = data[valid]\n",
    "    validation_labels = labels[valid]\n",
    "    \n",
    "    # K.clear_session()\n",
    "    # model = None # Clearing the NN.\n",
    "    model = classifier\n",
    "    f1_score = train_and_evaluate__model(model, train_data, train_labels, validation_data, validation_labels, test_features, np.asarray(test_y))\n",
    "    f1_scores.append(f1_score)\n",
    "    del model; gc.collect()\n",
    "\n",
    "print('Mean F1 score is {} SD is {}'.format(np.mean(f1_score), np.std(f1_score)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
